# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H-ARD8TwUY64hAhZccQYMSkVsdU4v6A-
"""

!pip install ultralytics lap norfair

# ====== IMPORT LIBRARIES ======
import os
import cv2
import numpy as np
import pandas as pd
import norfair
from tqdm import tqdm
from ultralytics import YOLO
import joblib
import albumentations as A
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_auc_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import backend as K
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

# ====== FASE 2C: PARAMETER ADVANCED ======
TARGET_FPS = 30
TARGET_SEQUENCE_LENGTH = 41
BATCH_SIZE = 32
NUM_FEATURES = 13  # Ditingkatkan dari 7 ke 13 features

# Advanced parameters
LOCF_MAX_FRAMES = 5  # Maksimal 5 frame untuk LOCF forward-fill
EMA_ALPHA = 0.3      # Exponential Moving Average untuk smoothing tracking
PADDING_VALUE = -1.0 # Ganti dari 0 ke -1 untuk Masking layer

# Path
DATASET_DIR = "/content/drive/MyDrive/dataset"
METADATA_FILTER_PATH = "/content/metadata_filtered_final.csv" # Fixed path

print(f"🎯 FASE 2C ADVANCED FEATURES:")
print(f"📊 Features: {NUM_FEATURES} (vs 7 di Fase 2B)")
print(f"🔄 LOCF max frames: {LOCF_MAX_FRAMES}")
print(f"📈 EMA alpha: {EMA_ALPHA}")
print(f"🎭 Padding value: {PADDING_VALUE} (with Masking)")

# ====== FOCAL LOSS IMPLEMENTATION ======
def focal_loss(gamma=2.0, alpha=0.25):
    """
    Focal Loss untuk mengatasi class imbalance dengan fokus pada hard examples.
    gamma: focusing parameter (higher = lebih fokus pada hard examples)
    alpha: balancing parameter untuk class imbalance
    """
    def focal_loss_fixed(y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)

        # Calculate focal loss
        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)

        focal_loss_value = -alpha_t * K.pow((1 - p_t), gamma) * K.log(p_t)
        return K.mean(focal_loss_value)

    return focal_loss_fixed

print("✅ Focal Loss function defined")

# ====== LOAD YOLO MODEL & METADATA ======
yolo_model = YOLO("yolov8n.pt")
print("✅ YOLO model loaded")

# Load metadata filter
try:
    if os.path.exists(METADATA_FILTER_PATH):
        metadata_df = pd.read_csv(METADATA_FILTER_PATH)
        valid_files = set(metadata_df['filename'].tolist())
        print(f"✅ Metadata filter loaded: {len(valid_files)} valid files")
        print(f"📊 Class distribution:")
        print(metadata_df['class_name'].value_counts())
    else:
        valid_files = set()
        print(f"⚠️ No filter - will process all files")
except Exception as e:
    valid_files = set()
    print(f"❌ Error loading filter: {e}")

# ====== ADVANCED PROCESS_VIDEO FUNCTION (FASE 2C) ======
def process_video_v3_advanced(video_path, label, model, sequence_length=61, target_fps=15, do_augment=False):
    """
    Advanced process_video function dengan fitur Fase 2C:
    - Presence feature (0/1)
    - LOCF untuk missing detection
    - Fitur turunan: area, dArea, dAspect, dCenterY, vy_pos, ax
    - EMA smoothing untuk tracking stability
    - Padding dengan -1 untuk Masking layer
    """
    tracker = norfair.Tracker(distance_function="iou", distance_threshold=0.7)

    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return None, None, 0, 0

        # Get video properties
        original_fps = cap.get(cv2.CAP_PROP_FPS)

        # Read all frames
        frames = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frames.append(frame)
        cap.release()

        if len(frames) == 0:
            return None, None, 0, 0

        # FPS normalization
        if original_fps > 0 and original_fps != target_fps:
            duration_seconds = len(frames) / original_fps
            target_frame_count = int(duration_seconds * target_fps)

            if target_frame_count > 0:
                frame_indices = np.linspace(0, len(frames)-1, target_frame_count).astype(int)
                frames = [frames[i] for i in frame_indices]

        if do_augment:
            frames = [augment_frame(f) for f in frames]

        # Initialize tracking variables
        sequence = []
        frame_count = len(frames)
        person_detected_total = 0

        if frame_count == 0:
            return None, None, 0, 0

        frame_width = frames[0].shape[1]
        frame_height = frames[0].shape[0]

        # ====== ADVANCED TRACKING VARIABLES ======
        prev_features = None  # Untuk LOCF
        consecutive_no_detection = 0

        # EMA tracking smoothing
        ema_nx, ema_ny, ema_nw, ema_nh = None, None, None, None

        # Temporal derivatives storage
        prev_area = None
        prev_aspect = None
        prev_center_y = None
        prev_vy = None

        for frame_idx, frame in enumerate(frames):
            # YOLO detection
            results = model(frame, conf=0.35, iou=0.4, verbose=False)  # Raised conf threshold
            detections = []

            # Filter person detections
            persons = [r for r in results[0].boxes if int(r.cls[0]) == 0]

            if persons:
                # Reset consecutive no detection counter
                consecutive_no_detection = 0
                person_detected_total += 1

                # Get best detection
                best_box = max(persons, key=lambda b: b.conf[0])
                x1, y1, x2, y2 = best_box.xyxy[0].cpu().numpy()

                # Raw bbox coordinates
                raw_nx = x1 / frame_width
                raw_ny = y1 / frame_height
                raw_nw = (x2 - x1) / frame_width
                raw_nh = (y2 - y1) / frame_height

                # ====== EMA SMOOTHING ======
                if ema_nx is None:  # First detection
                    ema_nx, ema_ny, ema_nw, ema_nh = raw_nx, raw_ny, raw_nw, raw_nh
                else:
                    ema_nx = EMA_ALPHA * raw_nx + (1 - EMA_ALPHA) * ema_nx
                    ema_ny = EMA_ALPHA * raw_ny + (1 - EMA_ALPHA) * ema_ny
                    ema_nw = EMA_ALPHA * raw_nw + (1 - EMA_ALPHA) * ema_nw
                    ema_nh = EMA_ALPHA * raw_nh + (1 - EMA_ALPHA) * ema_nh

                # Use smoothed values
                nx, ny, nw, nh = ema_nx, ema_ny, ema_nw, ema_nh
                presence = 1.0

                detections.append(
                    norfair.Detection(points=np.array([[x1, y1], [x2, y2]]))
                )
            else:
                # No detection - check LOCF possibility
                consecutive_no_detection += 1

                if consecutive_no_detection <= LOCF_MAX_FRAMES and prev_features is not None:
                    # ====== LOCF (Last Observation Carried Forward) ======
                    nx = prev_features['nx']
                    ny = prev_features['ny']
                    nw = prev_features['nw']
                    nh = prev_features['nh']
                    presence = 0.0  # Mark as interpolated

                    # Update EMA with LOCF values to maintain continuity
                    if ema_nx is not None:
                        ema_nx, ema_ny, ema_nw, ema_nh = nx, ny, nw, nh
                else:
                    # Beyond LOCF limit or no previous features
                    nx = ny = nw = nh = 0.0
                    presence = 0.0
                    # Reset EMA
                    ema_nx = ema_ny = ema_nw = ema_nh = None

            # ====== ADVANCED FEATURE EXTRACTION ======

            # Basic features
            center_x = nx + nw / 2
            center_y = ny + nh / 2
            area = nw * nh
            aspect_ratio = nh / (nw + 1e-6)

            # Temporal derivatives
            if frame_idx == 0:
                vx = vy = 0.0
                dArea = dAspect = dCenterY = 0.0
                vy_pos = ax = 0.0
            else:
                # Velocity
                vx = center_x - prev_features['center_x'] if prev_features else 0.0
                vy = center_y - prev_features['center_y'] if prev_features else 0.0

                # Derivatives
                dArea = (area - prev_area) if prev_area is not None else 0.0
                dAspect = (aspect_ratio - prev_aspect) if prev_aspect is not None else 0.0
                dCenterY = (center_y - prev_center_y) if prev_center_y is not None else 0.0

                # Advanced velocity features
                vy_pos = max(0.0, vy)  # Only positive (downward) vertical velocity
                ax = (vy - prev_vy) if prev_vy is not None else 0.0  # Vertical acceleration

            # ====== 13-FEATURE VECTOR ======
            # [nx, ny, nw, nh, vx, vy, aspect_ratio, presence, area, dArea, dAspect, dCenterY, vy_pos, ax]
            # Removed ax to make it 13 features total
            features = [
                nx, ny, nw, nh,           # 0-3: Basic bbox (normalized)
                vx, vy,                   # 4-5: Velocity
                aspect_ratio,             # 6: Aspect ratio
                presence,                 # 7: Detection presence (0/1)
                area,                     # 8: Bbox area
                dArea,                    # 9: Area change rate
                dAspect,                  # 10: Aspect ratio change rate
                dCenterY,                 # 11: Vertical position change
                vy_pos                    # 12: Positive vertical velocity
            ]

            sequence.append(features)

            # ====== UPDATE PREVIOUS VALUES ======
            if presence > 0:  # Only update with real detections
                prev_features = {
                    'nx': nx, 'ny': ny, 'nw': nw, 'nh': nh,
                    'center_x': center_x, 'center_y': center_y
                }

            prev_area = area
            prev_aspect = aspect_ratio
            prev_center_y = center_y
            prev_vy = vy

            # Update tracker (for consistency)
            tracked_objects = tracker.update(detections=detections)

        # ====== SEQUENCE LENGTH NORMALIZATION WITH PADDING=-1 ======
        if len(sequence) < sequence_length:
            # Padding dengan -1 untuk Masking layer
            padding_needed = sequence_length - len(sequence)
            padding_features = [PADDING_VALUE] * NUM_FEATURES
            sequence.extend([padding_features] * padding_needed)
        elif len(sequence) > sequence_length:
            sequence = sequence[:sequence_length]

        return np.array(sequence, dtype=np.float32), label, frame_count, person_detected_total

    except Exception as e:
        print(f"❌ Error processing {video_path}: {e}")
        return None, None, 0, 0

print("✅ Advanced process_video_v3 function loaded with 13 features")
print(f"📋 Features: [nx, ny, nw, nh, vx, vy, aspect_ratio, presence, area, dArea, dAspect, dCenterY, vy_pos]")

# ====== AUGMENTATION FUNCTIONS (SAME AS BEFORE) ======
augment = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.3),
    A.MotionBlur(p=0.2),
    A.Rotate(limit=10, p=0.3)
])

def augment_frame(frame):
    return augment(image=frame)["image"]

print("✅ Augmentation functions loaded")

# ====== DATASET PROCESSING WITH ADVANCED FEATURES ======
X_all, y_all = [], []
processed_count = 0
error_count = 0

class_map = {"nonJatuh": 0, "jatuh": 1}

print(f"🚀 Starting Fase 2C dataset processing with advanced features...")
print(f"📊 Expected features per frame: {NUM_FEATURES}")

for label_name, label_value in class_map.items():
    class_path = os.path.join(DATASET_DIR, label_name)
    if not os.path.exists(class_path):
        print(f"⚠️ Class directory not found: {class_path}")
        continue

    # Get and filter video files
    all_video_files = [f for f in os.listdir(class_path) if f.lower().endswith((".mp4", ".avi", ".mov"))]

    if valid_files:
        filtered_video_files = [f for f in all_video_files if f in valid_files]
        print(f"📁 {label_name}: {len(filtered_video_files)}/{len(all_video_files)} files passed filter")
    else:
        filtered_video_files = all_video_files
        print(f"📁 {label_name}: Processing all {len(filtered_video_files)} files")

    # Process with advanced function
    for filename in tqdm(filtered_video_files, desc=f"Processing {label_name} (Fase 2C)"):
        video_path = os.path.join(class_path, filename)

        features, label, frame_count, person_count = process_video_v3_advanced(
            video_path, label_value, yolo_model,
            sequence_length=TARGET_SEQUENCE_LENGTH,
            target_fps=TARGET_FPS,
            do_augment=False
        )

        if features is not None:
            # Validate feature dimensions
            if features.shape == (TARGET_SEQUENCE_LENGTH, NUM_FEATURES):
                X_all.append(features)
                y_all.append(label)
                processed_count += 1

                if processed_count % 50 == 0:
                    print(f"✅ Processed {processed_count} files...")
            else:
                print(f"⚠️ Invalid feature shape {features.shape} for {filename}")
                error_count += 1
        else:
            error_count += 1

print(f"\n📊 FASE 2C PROCESSING SUMMARY:")
print(f"✅ Successfully processed: {processed_count} videos")
print(f"❌ Errors: {error_count} videos")
print(f"📋 Features shape: {np.array(X_all).shape if X_all else 'None'}")
print(f"📋 Label distribution: {np.bincount(y_all) if y_all else 'None'}")

# ====== CONVERT TO NUMPY & SAVE ======
if X_all and y_all:
    X = np.array(X_all, dtype=np.float32)
    y = np.array(y_all, dtype=np.int32)

    print(f"📊 FASE 2C FINAL DATASET:")
    print(f"X (features): {X.shape}")
    print(f"y (labels): {y.shape}")
    print(f"📋 Class distribution: nonJatuh: {np.sum(y==0)}, jatuh: {np.sum(y==1)}")
    print(f"⚖️ Imbalance ratio: {np.sum(y==0) / np.sum(y==1):.2f}:1")

    # Feature statistics
    print(f"\n📈 FEATURE STATISTICS:")
    print(f"Features range: [{X.min():.3f}, {X.max():.3f}]")
    print(f"Padding values (-1): {np.sum(X == PADDING_VALUE)} / {X.size} ({np.sum(X == PADDING_VALUE)/X.size*100:.1f}%)")
    print(f"Presence feature (index 7) - Detection ratio: {np.mean(X[:,:,7]):.3f}")

    # Save dataset
    np.savez('fase_2c_advanced_dataset.npz', X=X, y=y)
    print(f"💾 Advanced dataset saved as 'fase_2c_advanced_dataset.npz'")
else:
    print(f"❌ No data processed successfully!")

# ====== LOAD DATASET FROM NPZ ======
if os.path.exists("/content/fase_2c_advanced_dataset.npz"):
    data = np.load("/content/fase_2c_advanced_dataset.npz")
    X = data["X"]
    y = data["y"]
    print(f"✅ Dataset loaded from NPZ: X={X.shape}, y={y.shape}")
else:
    print("❌ NPZ dataset not found!")
    X, y = None, None

# ====== TRAIN/VAL/TEST SPLIT ======
if X is not None and y is not None:
    # Stratified split
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )

    print(f"📊 DATASET SPLIT:")
    print(f"Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
    print(f"Val:   {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)")
    print(f"Test:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")

    print(f"\n📋 CLASS DISTRIBUTION:")
    for split_name, y_split in [("Train", y_train), ("Val", y_val), ("Test", y_test)]:
        non_jatuh = np.sum(y_split == 0)
        jatuh = np.sum(y_split == 1)
        print(f"{split_name}: nonJatuh: {non_jatuh}, jatuh: {jatuh} (ratio: {non_jatuh/jatuh:.2f}:1)")
else:
    print(f"❌ Cannot split dataset - no data available")

# ====== ADVANCED FEATURE STANDARDIZATION ======
if 'X_train' in locals():
    print(f"📊 Standardizing {NUM_FEATURES} features...")

    # Create mask for non-padding values
    train_mask = X_train != PADDING_VALUE

    # Separate scaler for each feature (except presence which is binary)
    scalers = {}

    def scale_features_advanced(X_data, fit_mode=False):
        X_scaled = X_data.copy()

        for feature_idx in range(NUM_FEATURES):
            if feature_idx == 7:  # Skip presence feature (binary)
                continue

            # Get mask for this feature in the current data
            current_mask = X_data[:, :, feature_idx] != PADDING_VALUE

            if fit_mode:
                # Get non-padding values for this feature only for fitting (from X_train)
                fit_mask = train_mask[:, :, feature_idx]
                feature_values_to_fit = X_train[:, :, feature_idx][fit_mask]


                if len(feature_values_to_fit) > 0:
                    scaler = StandardScaler()
                    scaler.fit(feature_values_to_fit.reshape(-1, 1))
                    scalers[feature_idx] = scaler

                    # Transform the current data using the fitted scaler
                    if np.any(current_mask):
                        feature_values_to_transform = X_data[:, :, feature_idx][current_mask]
                        scaled_values = scaler.transform(feature_values_to_transform.reshape(-1, 1)).flatten()
                        X_scaled[:, :, feature_idx][current_mask] = scaled_values # Apply transformation to non-padding values
            else:
                # Transform using existing scaler
                if feature_idx in scalers:
                    if np.any(current_mask):
                        feature_values_to_transform = X_data[:, :, feature_idx][current_mask]
                        scaled_values = scalers[feature_idx].transform(feature_values_to_transform.reshape(-1, 1)).flatten()
                        X_scaled[:, :, feature_idx][current_mask] = scaled_values # Apply transformation to non-padding values


        return X_scaled

    # Fit and transform training data
    X_train_scaled = scale_features_advanced(X_train, fit_mode=True)

    # Transform validation and test data
    X_val_scaled = scale_features_advanced(X_val, fit_mode=False)
    X_test_scaled = scale_features_advanced(X_test, fit_mode=False)

    print(f"✅ Advanced feature standardization completed")
    print(f"📊 Scalers created for {len(scalers)} features (skipped presence)")
    print(f"📈 Scaled ranges: [{np.nanmin(np.where(X_train_scaled != PADDING_VALUE, X_train_scaled, np.nan)):.3f}, {np.nanmax(np.where(X_train_scaled != PADDING_VALUE, X_train_scaled, np.nan)):.3f}]")


    # Save scalers
    joblib.dump(scalers, 'fase_2c_feature_scalers.pkl')
    print(f"💾 Feature scalers saved")
else:
    print(f"❌ Cannot standardize features - no training data available")

X_train_scaled

# ====== ADVANCED MODEL ARCHITECTURE (BiLSTM + MASKING) ======
if 'X_train_scaled' in locals():
    # Calculate class weights
    from sklearn.utils.class_weight import compute_class_weight

    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(y_train),
        y=y_train
    )
    class_weight_dict = dict(zip(np.unique(y_train), class_weights))

    print(f"⚖️ Class weights: {class_weight_dict}")

    # ====== ADVANCED BiLSTM MODEL WITH MASKING ======
    model = Sequential([
        # Masking layer untuk handle padding -1
        Masking(mask_value=PADDING_VALUE, input_shape=(TARGET_SEQUENCE_LENGTH, NUM_FEATURES)),

        # BiLSTM layers untuk capture temporal patterns dari kedua arah
        Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),
        Dropout(0.3),

        Bidirectional(LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),
        Dropout(0.4),

        # Dense layers dengan regularization
        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.5),

        Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')
    ])

    # Compile dengan Focal Loss untuk handle class imbalance
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',  # Focal loss untuk hard examples
        metrics=['accuracy', 'precision', 'recall']
    )

    print(f"✅ Advanced BiLSTM model with Masking compiled")
    print(f"🎭 Masking value: {PADDING_VALUE}")
    print(f"🔥 Loss function: Focal Loss (gamma=2.0, alpha=0.25)")
    model.summary()
else:
    print(f"❌ Cannot create model - no scaled training data available")

# ====== ADVANCED TRAINING CALLBACKS ======
if 'model' in locals():
    callbacks = [
      EarlyStopping(
          monitor='val_accuracy',
          patience=30,
          restore_best_weights=True,
          verbose=1,
          mode='max'   # penting, karena accuracy makin besar makin bagus
      ),
      ReduceLROnPlateau(
          monitor='val_accuracy',
          factor=0.5,
          patience=10,
          min_lr=1e-6,
          verbose=1,
          mode='max'   # sama, karena target akurasi maksimum
      ),
      tf.keras.callbacks.ModelCheckpoint(
          'fase_2c_best_model.keras',
          monitor='val_accuracy',
          save_best_only=True,
          verbose=1,
          mode='max'
      )
  ]

    print(f"✅ Advanced training callbacks configured")
    print(f"📊 Patience: EarlyStopping=20, ReduceLR=10")
else:
    print(f"❌ Cannot configure callbacks - no model available")

# ====== MODEL TRAINING (FASE 2C) ======
if 'model' in locals() and 'X_train_scaled' in locals():
    print(f"🚀 Starting Fase 2C advanced model training...")
    print(f"📊 Training: {len(X_train_scaled)} samples, Validation: {len(X_val_scaled)} samples")
    print(f"🎯 Features: {NUM_FEATURES} (vs 7 in Fase 2B)")
    print(f"🔥 Loss: Focal Loss (focuses on hard examples)")
    print(f"🎭 Masking: Enabled for padding value {PADDING_VALUE}")

    history = model.fit(
        X_train_scaled, y_train,
        validation_data=(X_val_scaled, y_val),
        epochs=150,  # Increased epochs for focal loss
        batch_size=BATCH_SIZE,
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )

    print(f"✅ Fase 2C training completed!")

    # Save final model
    model.save('fase_2c_final_model.keras')
    print(f"💾 Final model saved as 'fase_2c_final_model.keras'")

    # Save training history
    import pickle
    with open('fase_2c_training_history.pkl', 'wb') as f:
        pickle.dump(history.history, f)
    print(f"💾 Training history saved")
else:
    print(f"❌ Cannot train model - requirements not met")

# ====== COMPREHENSIVE MODEL EVALUATION ======
if 'model' in locals() and 'X_test_scaled' in locals():
    print(f"📊 EVALUATING FASE 2C ADVANCED MODEL...")

    # Get predictions
    y_pred_proba = model.predict(X_test_scaled)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()

    # Calculate metrics
    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test_scaled, y_test, verbose=0)

    # Additional metrics
    from sklearn.metrics import f1_score, precision_recall_fscore_support, average_precision_score

    f1 = f1_score(y_test, y_pred)
    precision, recall, f1_detailed, support = precision_recall_fscore_support(y_test, y_pred, average=None)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    pr_auc = average_precision_score(y_test, y_pred_proba)

    print(f"\n🎯 FASE 2C ADVANCED RESULTS:")
    print(f"═" * 50)
    print(f"Test Loss (Focal): {test_loss:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")
    print(f"Test Precision: {test_precision:.4f}")
    print(f"Test Recall: {test_recall:.4f}")
    print(f"Test F1 Score: {f1:.4f}")
    print(f"Test ROC AUC: {roc_auc:.4f}")
    print(f"Test PR AUC: {pr_auc:.4f}")

    print(f"\n📋 PER-CLASS DETAILED METRICS:")
    class_names = ['nonJatuh', 'jatuh']
    for i, class_name in enumerate(class_names):
        print(f"{class_name:>8}: Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1: {f1_detailed[i]:.4f}, Support: {support[i]}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n📊 CONFUSION MATRIX:")
    print(f"     Pred:  nonJatuh  jatuh")
    print(f"True:")
    print(f"nonJatuh:     {cm[0,0]:3d}     {cm[0,1]:3d}")
    print(f"jatuh:        {cm[1,0]:3d}     {cm[1,1]:3d}")

    # Calculate improvement metrics
    false_positive_rate = cm[0,1] / (cm[0,0] + cm[0,1])
    false_negative_rate = cm[1,0] / (cm[1,0] + cm[1,1])

    print(f"\n🎯 ERROR ANALYSIS:")
    print(f"False Positive Rate: {false_positive_rate:.4f} ({cm[0,1]} / {cm[0,0] + cm[0,1]})")
    print(f"False Negative Rate: {false_negative_rate:.4f} ({cm[1,0]} / {cm[1,0] + cm[1,1]})")

    # Visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Confusion Matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names, ax=axes[0,0])
    axes[0,0].set_title('Confusion Matrix - Fase 2C Advanced')
    axes[0,0].set_xlabel('Predicted')
    axes[0,0].set_ylabel('Actual')

    # Prediction distribution
    axes[0,1].hist(y_pred_proba[y_test==0], bins=20, alpha=0.7, label='nonJatuh', color='blue')
    axes[0,1].hist(y_pred_proba[y_test==1], bins=20, alpha=0.7, label='jatuh', color='red')
    axes[0,1].axvline(x=0.5, color='black', linestyle='--', label='Threshold')
    axes[0,1].set_title('Prediction Probability Distribution')
    axes[0,1].set_xlabel('Probability')
    axes[0,1].set_ylabel('Count')
    axes[0,1].legend()

    # Training history (if available)
    if 'history' in locals():
        axes[1,0].plot(history.history['loss'], label='Train Loss')
        axes[1,0].plot(history.history['val_loss'], label='Val Loss')
        axes[1,0].set_title('Training Loss (Focal Loss)')
        axes[1,0].set_xlabel('Epoch')
        axes[1,0].set_ylabel('Loss')
        axes[1,0].legend()

        axes[1,1].plot(history.history['accuracy'], label='Train Accuracy')
        axes[1,1].plot(history.history['val_accuracy'], label='Val Accuracy')
        axes[1,1].set_title('Training Accuracy')
        axes[1,1].set_xlabel('Epoch')
        axes[1,1].set_ylabel('Accuracy')
        axes[1,1].legend()

    plt.tight_layout()
    plt.savefig('fase_2c_evaluation_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Classification report
    print(f"\n📋 DETAILED CLASSIFICATION REPORT:")
    print(classification_report(y_test, y_pred, target_names=class_names))

    # Save evaluation results
    eval_results = {
        'fase': '2C_advanced',
        'features': NUM_FEATURES,
        'architecture': 'BiLSTM + Masking + Focal Loss',
        'test_loss': float(test_loss),
        'test_accuracy': float(test_accuracy),
        'test_precision': float(test_precision),
        'test_recall': float(test_recall),
        'test_f1': float(f1),
        'test_roc_auc': float(roc_auc),
        'test_pr_auc': float(pr_auc),
        'confusion_matrix': cm.tolist(),
        'false_positive_rate': float(false_positive_rate),
        'false_negative_rate': float(false_negative_rate),
        'per_class_precision': precision.tolist(),
        'per_class_recall': recall.tolist(),
        'per_class_f1': f1_detailed.tolist(),
        'per_class_support': support.tolist(),
        'improvements': {
            'presence_feature': True,
            'locf_interpolation': True,
            'temporal_derivatives': True,
            'ema_smoothing': True,
            'masking_layer': True,
            'bidirectional_lstm': True,
            'focal_loss': True
        }
    }

    import json
    with open('fase_2c_evaluation_results.json', 'w') as f:
        json.dump(eval_results, f, indent=2)

    print(f"💾 Comprehensive evaluation results saved")
    print(f"\n🎉 FASE 2C COMPLETED! Ready for next phase.")
else:
    print(f"❌ Cannot evaluate model - requirements not met")

